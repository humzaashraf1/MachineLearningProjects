{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installs for NVIDIA GeForce RTX 3080\n",
    "#pip install transformers\n",
    "#pip install scikit-learn\n",
    "#pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "#separately download cuda_12.3.1_546.12_windows & follow express install instructions\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Print the name of the GPU\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available, using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face implementation of [10.34133/research.0004](https://doi.org/10.1101/2020.07.12.199554) by Rostlab/prot_bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from excel spreadsheet\n",
    "df = pd.read_excel(\"Aggregated.xlsx\")\n",
    "\n",
    "sequence_lengths = df['sequence'].apply(len)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(sequence_lengths, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Sequence Lengths')\n",
    "plt.xlabel('Length of Sequences')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel, TrainingArguments, AutoTokenizer, TrainingArguments, AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "class ProteinSequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences, targets, tokenizer, max_length):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = str(self.sequences[idx])\n",
    "        target = self.targets[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sequence,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'Rostlab/prot_bert_bfd_localization'\n",
    "\n",
    "class ProteinClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(ProteinClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.classifier = nn.Sequential(nn.Dropout(p=0.4),\n",
    "                                        nn.Linear(self.bert.config.hidden_size, n_classes),\n",
    "                                        nn.Tanh())\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init.xavier_uniform_(self.classifier[1].weight)\n",
    "        init.constant_(self.classifier[1].bias, 0)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "        return self.classifier(output.pooler_output)\n",
    "\n",
    "\n",
    "# Load the ProtBERT tokenizer and model\n",
    "pretrained_model_name = \"Rostlab/prot_bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "model = ProteinClassifier(1)\n",
    "\n",
    "# Extract sequences and labels\n",
    "X = df['sequence'].tolist()\n",
    "y = df['label'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "X_train_preprocessed = [' '.join(seq) for seq in X_train]\n",
    "X_test_preprocessed = [' '.join(seq) for seq in X_test]\n",
    "\n",
    "# Tokenize and encode the data\n",
    "max_length = 60  # based on distribution of seq lengths\n",
    "train_encodings = tokenizer(X_train_preprocessed, truncation=True, padding=True, max_length=max_length)\n",
    "valid_encodings = tokenizer(X_test_preprocessed, truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert tokenized data into a torch Dataset\n",
    "train_dataset = ProteinSequenceDataset(sequences=X_train_preprocessed, targets=y_train, tokenizer=tokenizer, max_length=max_length)\n",
    "valid_dataset = ProteinSequenceDataset(sequences=X_test_preprocessed, targets=y_test, tokenizer=tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sample sequence\n",
    "sequence = \"S D P K I G D G C F G L P L D H I G S V S G L G C N R P V Q N R P K K\"\n",
    "tokenized_sequence = tokenizer(sequence)\n",
    "\n",
    "# Print the tokenized sequence\n",
    "print(\"Original sequence:\", sequence)\n",
    "print(\"Tokenized sequence:\", tokenized_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(valid_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.2) #changed from 1e-5 and 0.2\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        labels = labels.unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        labels = labels.to(outputs.dtype)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        predictions = (probabilities > 0.5).float()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        correct_train += (predictions == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "    \n",
    "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracy = correct_train / total_train\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Testing\n",
    "    model.eval()\n",
    "    epoch_test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            labels = labels.unsqueeze(1)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            labels = labels.to(outputs.dtype)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_test_loss += loss.item()\n",
    "\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predictions = (probabilities > 0.5).float()\n",
    "\n",
    "            correct_test += (predictions == labels).sum().item()\n",
    "            total_test += labels.size(0)\n",
    "\n",
    "    avg_test_loss = epoch_test_loss / len(test_loader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    test_accuracy = correct_test / total_test\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Testing Loss: {avg_test_loss:.4f}, Testing Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "\n",
    "# Creating subplots with 1 row and 2 columns\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Plotting the loss curves\n",
    "axs[0].plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "axs[0].plot(range(1, num_epochs + 1), test_losses, label='Testing Loss')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plotting the accuracy curves\n",
    "axs[1].plot(range(1, num_epochs + 1), train_accuracies, label='Training Accuracy')\n",
    "axs[1].plot(range(1, num_epochs + 1), test_accuracies, label='Testing Accuracy')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in (test_loader):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    labels = labels.unsqueeze(1)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask)\n",
    "    labels = labels.to(outputs.dtype)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    epoch_test_loss += loss.item()\n",
    "\n",
    "    probabilities = torch.sigmoid(outputs)\n",
    "    predictions = (probabilities > 0.5).float()\n",
    "\n",
    "    correct_test += (predictions == labels).sum().item()\n",
    "    total_test += labels.size(0)\n",
    "    \n",
    "    # Print the original sequence, the ground truth label, and the predicted label\n",
    "    original_sequence = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    ground_truth_label = labels[0].item()\n",
    "    predicted_label = predictions[0].item()\n",
    "    print(f\"Original Sequence: {original_sequence}\") \n",
    "    print(f\"Ground Truth Label: {ground_truth_label}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
